```{r setup, echo=FALSE} knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE) ``` ```{css style settings, echo = FALSE} blockquote { padding: 10px 20px; margin: 0 0 20px; font-size: 14px; border-left: 5px solid #eee; } ``` This script uses the following packages: ```{r echo=T} # library(ggplot2) # library(tidyr) # library(lme4) # library(data.table) # library(parallel) # library(knitr) # library(kableExtra) library(purrr) library(foreach) library(doSNOW) ``` ```{r} setwd('~/Desktop/VWP Tutorial/Data/') d <- read.csv("SC_FinalData_n57.csv") d=d %>% filter(Condition %in% c('pre-switch','post-switch'), percentMissingFrames < 50) str(d) head(d) ``` ## Overview In these analyses, we will test for significance using cluster-based permutation analyses. In these analyses, we will fit models (either t-tests or logistic mixed effects) for each time bin throughout the entire critical window (i.e., every 33ms from 300 to 1800 ms after the onset of the critical word). We must take into account multiple comparisons, however, because if there is in fact no difference and we complete 46 tests, we will find a significant effect on average `r .05*46` times (i.e., 5% of 46). Bonferonni corrections are often used, but are too conservative. An alternative method is to randomly shuffle our data 1,000 times, refitting the models for each time bin each time. For each permutation, we will extract the largest cluster of contiguous significant effects. We can then test the significance of our observed effect by comparing how likely we are to observe an effect of that magnitude (i.e., what percentage of the random permutations exceed the size of our observed effect). Randomization occurs at the Condition level. An important constraint is that any imbalances that naturally occur in the data are preserved. For instance, there are `r n_distinct(d$Tr.Num[d$Sub.Num=='501' & d$Condition=='pre-switch'])` pre-switch trials (1 excluded due to too much missing data) and `r n_distinct(d$Tr.Num[d$Sub.Num=='501' & d$Condition=='post-switch'])` post-switch trials for subject 501. Therefore, when randomly assigning trials to be in the pre-switch or post-switch condition in any given permutation for subject 501 we will always maintain this same imbalance. We'll start by using t tests for each time bin. These models rapidly fit and can therefore easily be repeated 1,000 times. In the final section I use logistic mixed effects for each time bin. This approach keeps the data at the individual trial level for each time bin (i.e., 1's and 0's). The cost, however, is that these models are more complex and require longer to fit. This difference in time is trivial when fitting just one model, but quickly accumulates when conducting many permutations. A larger concern is that these models may fail to converge because we are drastically reducing the amount of data we have by conducting analyses for each time frame (i.e., a maximum of 16 data points per participant). As we'll see later, the current data set does not support this use of logistic mixed effects models (due to convergence issues). ## Plot timecourse Before we start analyzing the data, however, let's first plot a time course of the changes in children's accuracy during our critical window. Luckily the code is nearly identical to our mean accuracy code, except we are keeping `Time` when aggregating and changing our x-axis to be `Time` rather than `Condition`: ```{r echo=T} bySub=d %>% group_by(Sub.Num,Condition,TimeC) %>% filter(TimeC > -2000 & TimeC < 3000) %>% summarise( Trials=sum(!is.na(Accuracy)), Accuracy=mean(Accuracy,na.rm=T)) ``` ```{r} kable(head(bySub[bySub$TimeC >= 0,],20), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` ```{r echo=T} byGroup=bySub %>% group_by(Condition,TimeC) %>% summarise( Subjects=sum(!is.na(Accuracy)), SD=sd(Accuracy,na.rm=TRUE), SE=SD/sqrt(Subjects), Accuracy=mean(Accuracy,na.rm=TRUE), lower=Accuracy-SE, upper=Accuracy+SE) byGroup$Condition = factor(byGroup$Condition,c("pre-switch","post-switch")) ``` ```{r} kable(head(byGroup[byGroup$TimeC >= 0,],20), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` ```{r echo=T} ggplot(byGroup, aes(x=TimeC, y=Accuracy, fill=Condition, color=Condition)) + geom_hline(aes(yintercept=0.5),linetype='solid',color='gray') + geom_line()+ geom_smooth(aes(ymin=lower, ymax=upper), stat="identity") + geom_vline(aes(xintercept=0), linetype="dashed", color="gray") + theme_bw(base_size=10) + coord_cartesian(xlim=c(-2000,3000), ylim=c(0,1.01),expand=F) + scale_x_continuous(breaks=seq(from=-2100,to=3000,by=300)) + scale_y_continuous(breaks=seq(from=0,to=1,by=.1)) + scale_fill_manual(values=c("dodgerblue","coral2")) + scale_color_manual(values=c("dodgerblue","coral2")) + labs(x='Time since target onset (in ms)',y='Proportion Looking to Target') + theme(legend.justification=c(1,0),legend.position=c(1,0),legend.background=element_rect(fill= NA, color=NA),legend.title=element_blank(),legend.text = element_text(size = 11)) ``` ## T-test clusters Let's first start off by conducting the `r n_distinct(d$TimeC[d$TimeC>=300 & d$TimeC<=1800])` t-tests, one for each time frame during our critical window (`r unique(d$TimeC[d$TimeC>=300 & d$TimeC<=1800])`). Technically we can conduct paired sample t-tests on the `bySub` data frame which is in long format. But, this can cause problems with some data sets - including the current data set. Notice how the number of subjects who contributed to the average accuracy for each time frame in the `byGroup` data frame changes over time. This is because some participants were not looking at either the target or distractor image for that exact time frame (e.g., 300ms) on all 8 trials. When all of a subject's accuracies have values of NA for that time frame in that condition their data is dropped from the analysis. This causes a problem, however, if that subject had accuracies for that time frame in the _other_ condition. A paired samples t-test won't work if there's data without its pair. So, we need to filter our data set to remove data for time frames where a subject only has data for one condition, but not the other. The easiest way to do this is to pivot or data frame into wide format like before: ```{r echo=T} bySubWide=bySub %>% select(-Trials) %>% pivot_wider(names_from=Condition,values_from=Accuracy) ``` And then to modify each column to remove data for a time frame where data is missing in the other column ```{r echo=T} bySubWide=bySubWide %>% mutate( pre=ifelse(is.na(`post-switch`),NA,`pre-switch`), post=ifelse(is.na(`pre-switch`),NA,`post-switch`) ) ``` Notice now how the original column `pre-switch` had accuracy values that are dropped (converted to NAs) for those rows where we are missing data in the `post-switch` column: ```{r} kable(head(bySubWide[is.na(bySubWide$`post-switch`),]),align='c',digits=2)%>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` Now we can conduct the paired-sample t-tests on all 46 frames in our critical window. > __note on syntax:__ > > adding `$statistic` to the end of the `t.test()` will return just the t-value from the t-test (we don't need all the other information) ```{r echo = T} ts <- bySubWide %>% filter(TimeC >=300 & TimeC<=1800) %>% group_by(TimeC) %>% summarise(vars = t.test(post, pre, paired=TRUE)$statistic) %>% mutate(Sig = ifelse(abs(vars)>2,"yes","no")) ``` ```{r} kable(head(ts,20), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` ```{r echo=T} ggplot(data=ts,aes(x=TimeC,y=vars,color=Sig)) + geom_point()+ theme_bw() + scale_color_manual(values=c("black","red")) + geom_hline(yintercept = 2) + geom_hline(yintercept = -2) + scale_x_continuous(breaks=seq(from=300,to=1800,by=300)) + labs(x='Time since target onset (in ms)',y='t value',title='Pre-switch vs. Post-switch') ``` For analysis purposes, we need a function to help us extract information about the largest cluster of consecutive significant t-tests. `get_max_cluster_window` identifies when each cluster starts, stops, and the sum of all the t values within each cluster (i.e., area under the curve=auc), then returns only the information for the cluster with the largest auc value. > __note on syntax:__ > > `rleid()` from the `data.table` package generates run-length data, which creates a new column of numbers that start with 1 and increment every time there is a change in value for the `Sig` column. <br> > for example, if `ts$Sig` had a value of `ns, ns, pos, pos, ns` then `data.table::rleid(ts$Sig)` would return `1, 1, 2, 2, 3` <br> > `which.max(abs(d_clust$auc)` selects only the row that corresponds to the cluster with the largest auc value ```{r echo=T} get_max_cluster_window <- function(m, threshold = 2){ colnames(m) <- c("time", "stat") d_clust <- m %>% mutate( sig=ifelse(stat >= threshold, "pos", ifelse(stat <= -threshold, "neg", "ns")), cluster=data.table::rleid(sig) ) %>% filter(sig != "ns") %>% group_by(cluster) %>% summarise( start=min(time), stop=max(time), auc=sum(stat)) max_cluster=data.frame(cluster=0,start=0,stop=0,auc=0) if(nrow(d_clust) > 0) {max_cluster = d_clust[which.max(abs(d_clust$auc)),]} return(max_cluster) } ``` ```{r echo=T} max_cluster=get_max_cluster_window(ts %>% select(TimeC,vars)) ``` Here's what `get_max_cluster_window()` returns when we give it our dataframe of 46 t tests: ```{r} kable(max_cluster,align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` It has the Time when the cluster started `533`, when it ends `833`, and the sum of the `r n_distinct(bySubWide$TimeC[bySubWide$TimeC>=533 & bySubWide$TimeC<=833])` consecutive t tests. Lastly, let's use this information to re-plot the time course with a horizontal line marking the significant effect of condition: ```{r} ggplot(byGroup, aes(x=TimeC, y=Accuracy, fill=Condition, color=Condition)) + geom_hline(aes(yintercept=0.5),linetype='solid',color='gray') + geom_line()+ geom_segment(x=max_cluster$start,xend=max_cluster$stop,y=.41,yend=.41,linetype='solid',color='black') + geom_smooth(aes(ymin=lower, ymax=upper), stat="identity") + geom_vline(aes(xintercept=0), linetype="dashed", color="gray") + theme_bw(base_size=10) + coord_cartesian(xlim=c(300,1800), ylim=c(.40,1.01),expand=F) + scale_x_continuous(breaks=seq(from=-900,to=2400,by=300)) + scale_y_continuous(breaks=seq(from=0,to=1,by=.1)) + scale_fill_manual(values=c("dodgerblue","coral2")) + scale_color_manual(values=c("dodgerblue","coral2")) + labs(x='Time since target onset (in ms)',y='Proportion Looking to Target',title='Condition Effect') + theme(legend.justification=c(1,0), plot.title=element_text(hjust=.5),legend.position=c(1,0),legend.background=element_rect(fill= NA, color=NA),legend.title=element_blank(),legend.text = element_text(size = 11)) ``` We still need to determine whether this cluster of t values is statistically significant. To do this, we will need to determine whether we would expect to find a cluster of this size less than 5% of the time when we conduct the same analyses on randomized data. ```{r} remove(bySub,bySubWide,byGroup,ts) ``` ### Randomization We need one more function to randomize the assignment of trials to condition. This function pseudo-randomly assigns each trial to be in either the pre-switch or the post-switch condition. With our two conditions, there are four possible scenarios that occur when randomizing: * a pre-switch trial is randomized to be in the __pre__-switch condition * a pre-switch trial is randomized to be in the __post__-switch condition * a post-switch trial is randomized to be in the __pre__-switch condition * a post-switch trial is randomized to be in the __post__-switch condition For any individual randomization, we may end up with a randomized condition that maintains most of the correct assignments (i.e., trials are randomized to be in the original condition). Other times we may end up with the opposite - a randomized condition that flips most of the correct assignments (i.e., trials are randomized to be in the "wrong" condition). But across many randomizations (we'll be doing 1,000) this will wash out so that, on average, any given trial will be assigned equally often in its "correct" vs. "wrong" condition following randomization. Moreover, for the average randomization, half of the trials will be correctly assigned and half will be incorrectly assigned. What this means is that the advantage we observe in our pre-switch trials should disappear when condition is randomized (since, on average, half of the trials in the randomized "pre-switch" condition will actually be post-switch trials). I say that this process is "pseudo-randomized", because we have the constraint that any imbalances in the number of trials per condition that naturally occurred for each subject are preserved. Recall that there are `r n_distinct(d$Tr.Num[d$Sub.Num=='501' & d$Condition=='pre-switch'])` pre-switch trials (1 excluded due to too much missing data) and `r n_distinct(d$Tr.Num[d$Sub.Num=='501' & d$Condition=='post-switch'])` post-switch trials for subject 501. Therefore, when randomly assigning trials to be in the pre-switch or post-switch condition in any given permutation for subject 501 we will always maintain this same imbalance. This function involves a fair bit of code, so I have embedded the syntax notes within the code: ```{r echo=T} shuffConditions=function(d,seed) { set.seed(seed) # set the random seed, which allows us to re-create the same randomization when re-running our analyses # create dataframe with one row per trial for each participant randCond=d %>% filter(percentMissingFrames < 50) %>% select(Sub.Num,Order,Tr.Num,Condition) %>% distinct() # determine number of trials in the pre-switch condition for each participant numPreTr=randCond %>% filter(Condition=='pre-switch') %>% group_by(Sub.Num) %>% summarise(n=length(Tr.Num)) # for each participant randomly select (w/o replacement) trials to match the number of trials in the pre-switch condition # assign these trials to be pre-switch in the randomized condition (CondR) # this randomized condition (CondR) will contain some mix of trials that are in fact from the pre-switch condition # and some trials that are from the post-switch condition new=randCond %>% group_by(Sub.Num) %>% nest() %>% left_join(numPreTr) %>% mutate(v = map2(data, n, ~sample_n(.x, .y))) %>% unnest(v) %>% select(-data,-n) %>% arrange(Sub.Num,Order,Tr.Num) %>% mutate(CondR='pre-switch') # merge the random selection with the full list of trials # assign trials without a randomized condition (CondR = NA) to be post-switch in the randomized condition (CondR) randCond=left_join(randCond,new) randCond$CondR[is.na(randCond$CondR)]='post-switch' d=left_join(d,randCond) } ``` ```{r} randCond=d %>% filter(percentMissingFrames < 50) %>% select(Sub.Num,Order,Tr.Num,Condition) %>% distinct() numPreTr=randCond %>% filter(Condition=='pre-switch') %>% group_by(Sub.Num) %>% summarise(n=length(Tr.Num)) new1=randCond %>% group_by(Sub.Num) %>% nest() %>% left_join(numPreTr) new2=new1 %>% mutate(v = map2(data, n, ~sample_n(.x, .y))) new3=new2 %>% unnest(v) new4=new3 %>% select(-data,-n) %>% arrange(Sub.Num,Order,Tr.Num) %>% mutate(CondR='pre-switch') ``` > __note on syntax:__ <br> > one element that I would like to highlight is the use of `nest()` and then `map2()` <br> > using `nest()` we have made a data frame with 1 row per participant and the column `data` is actually a dataframe with the order, trial number, and condition for each trial that participant completed ```{r} kable(head(new1,3), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` > then, using `map2()` we have randomly sampled _without replacement_ n number of rows (from the column `n`) from that data frame for each participant. this new data frame is saved as a new column `v` these are the trials that we are going to randomly assign to be in the 'pre-switch' condition. ```{r} kable(head(new2,3), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` > we use `unnest()` to re-convert the dataframes in column `v` into multiple rows. so there is no longer just one row of data per subject: ```{r} kable(head(new3,3), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` > finally, remove the column `data` that contained the nested data frame, which leaves us with a dataframe that only contains the trials that were randomly assigned to be in the pre-switch condition: ```{r} kable(head(new4,20), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` ```{r} remove(randCond,numPreTr,new1,new2,new3,new4) ``` Now we can repeat our previous steps in aggregating the data, but adding in our new `shuffConditions()` function, which is embedded in the tidyr pipeline. We are passing in the filtered data frame, which is the first required argument for this function. And we are manually passing in a value of 1 for the random seed (when using a for loop in the next section we will instead be passing in the `i` that increments from 1 to 1,000 during each iteration through the for loop). ```{r echo=T} bySub=d %>% filter(TimeC > -2000 & TimeC < 3000) %>% shuffConditions(1) %>% group_by(Sub.Num,CondR,TimeC) %>% summarise(Accuracy = mean(Accuracy,na.rm=T)) byGroup=bySub %>% group_by(CondR,TimeC) %>% summarise( Subjects=sum(!is.na(Accuracy)), SD=sd(Accuracy,na.rm=TRUE), SE=SD/sqrt(Subjects), Accuracy=mean(Accuracy,na.rm=TRUE), lower=Accuracy-SE, upper=Accuracy+SE) byGroup$CondR = factor(byGroup$CondR, c("pre-switch","post-switch")) bySubWide=bySub %>% pivot_wider(names_from=CondR,values_from=Accuracy) %>% mutate( pre = ifelse(is.na(`post-switch`),NA,`pre-switch`), post = ifelse(is.na(`pre-switch`),NA,`post-switch`) ) ts <- bySubWide %>% filter(TimeC >=300 & TimeC<=1800) %>% group_by(TimeC) %>% summarise(vars = t.test(post, pre, paired=TRUE)$statistic) %>% mutate(Sig = ifelse(abs(vars)>2,"yes","no")) ggplot(data=ts,aes(x=TimeC,y=vars,color=Sig)) + geom_point()+ theme_bw() + scale_color_manual(values=c("black","red")) + geom_hline(yintercept = 2) + geom_hline(yintercept = -2) + scale_x_continuous(breaks=seq(from=300,to=1800,by=300)) + labs(x='Time since target onset (in ms)',y='t-test (Coartic vs. Neutral)',title='Condition Effect (Shuffled)') ``` In this randomization there is not a single time frame during which the t test revealed a significant difference between our randomized pre-switch and post-switch conditions. As we'll see in the next section, however, this will not always be the case (some of our randomizations will yield clusters of significant t tests). ```{r} ggplot(byGroup, aes(x=TimeC, y=Accuracy, fill=CondR, color=CondR)) + geom_hline(aes(yintercept=0.5),linetype='solid',color='gray') + geom_line()+ geom_segment(x=max_cluster$start,xend=max_cluster$stop,y=.41,yend=.41,linetype='solid',color='black') + geom_smooth(aes(ymin=lower, ymax=upper), stat="identity") + geom_vline(aes(xintercept=0), linetype="dashed", color="gray") + theme_bw(base_size=10) + coord_cartesian(xlim=c(300,1800), ylim= c(.40,1.01),expand=F) + scale_x_continuous(breaks=seq(from=-900,to=2400,by=300)) + scale_y_continuous(breaks=seq(from=0,to=1,by=.1)) + scale_fill_manual(values=c("dodgerblue","coral2")) + scale_color_manual(values=c("dodgerblue","coral2")) + labs(x='Time since target onset (in ms)',y='Proportion Looking to Target',title='Condition Effect (Shuffled)') + theme(legend.justification=c(1,0), plot.title=element_text(hjust=.5),legend.position=c(1,0),legend.background=element_rect(fill= NA, color=NA),legend.title=element_blank(),legend.text = element_text(size = 11)) ``` ### Randomize x 1000 Great, now we just need to repeat this process 1,000 times. You may have noticed earlier that I leveraged the `map2()` function from the `purrr` package to complete a process over multiple inputs simultaneously. This parallel processing speeds things up. We'll want to use a similar logic here. Rather than running 46 t tests 1,000 times in order (i.e., one after the other), we can run several iterations of the 46 t tests simultaneously. How many we can run just depends on how many cores your computer has. My computer has 7 nodes. So we can run the code for multiple randomizations in parallel. ```{r echo=T} c1 <- makeCluster(parallel::detectCores()-1) registerDoSNOW(c1) c1 ```> note on __syntax__: > > we're using similar tidyr code as before, but embedding it within a `foreach(){}` loop that uses the `%dopar%` function from the `for_each` package to complete multiple iterations of our tidy r code in parallel; essentially running the first 7 iterations through our for loop (i.e., i=1 through 7) at the same time. > > the first part of the code uses `txtProgressBar()` to help us graphically track the progress of the 1,000 randomizations ```{r echo=T} # nrep=1000 # pb <- txtProgressBar(max=nrep, style=3) # progress <- function(n) setTxtProgressBar(pb, n) # opts <- list(progress=progress) # # ts_randCond=foreach (i = 1:nrep, .combine = c, .options.snow = opts, .packages = c("tidyverse","purrr")) %dopar% { # # bySub=d %>% # filter(TimeC > -2000 & TimeC < 3000) %>% # shuffConditions(i) %>% # group_by(Sub.Num,CondR,TimeC) %>% # summarise(Accuracy = mean(Accuracy,na.rm=T)) # # bySubWide=bySub %>% # pivot_wider(names_from=CondR,values_from=Accuracy) %>% # mutate( # pre = ifelse(is.na(`post-switch`),NA,`pre-switch`), # post = ifelse(is.na(`pre-switch`),NA,`post-switch`)) # # ts <- bySubWide %>% # filter(TimeC >=300 & TimeC<=1800) %>% # group_by(TimeC) %>% # summarise(vars = t.test(post, pre, paired=TRUE)$statistic) %>% # mutate(Sig = ifelse(abs(vars)>2,"yes","no")) # # ts=ts %>% select(TimeC,vars) # # max_cluster_window=get_max_cluster_window(ts) # # return(max_cluster_window$auc) # # } ``` The output from this is a list with 1,000 values. Each value is the area under the curve (auc) for the largest cluster of significant t values for one randomization: ```{r} load('ts_randCond.Rdata') ``` ```{r} kable(head(ts_randCond,10), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` This code takes my computer somewhere between 1 to 2 minutes to complete. To speed up the process of knitting the markdown file, let's save the output as an .RData file to reload in future iterations and comment out the original code: ```{r echo=T} # save(ts_randCond,file='ts_randCond.Rdata') # load('ts_randCond.Rdata') ``` Now we can conduct our test of significance by determining how many times the auc value of the largest cluster of significant t tests in our actual data exceeds the maximum auc values of the 1,000 randomizations. ```{r echo=T} p_obs <- mean(abs(ts_randCond)> abs(max_cluster$auc)) t_crit_pos <- quantile(ts_randCond, probs = 0.975) t_crit_neg <- quantile(ts_randCond, probs = 0.025) ggplot(mapping = aes(x = ts_randCond)) + geom_density(fill = "green", alpha = .5) + geom_vline(xintercept = t_crit_pos) + geom_vline(xintercept = t_crit_neg) + geom_vline(xintercept = max_cluster$auc, colour = "blue") + labs( title = "Condition Effect, 300-1800 ms", x = "Largest Cluster (1000 Permutations)" ) ``` With the raw data, children's accuracy in fixating the target object was higher for pre-switch compared to post-switch trials during a window from `r max_cluster$start`ms to `r max_cluster$stop`ms after the onset of the critical word. The size of this effect (i.e., area under the curve) is `r round(max_cluster$auc,digits=2)`. When reshuffling the data (randomly re-assigning trials for a child to be in the pre- vs. post-switch condition), we observe an effect of this size `r paste(p_obs*100,'%',sep='')` of the time. Therefore, we can conclude that the difference we observe between the trials before (pre-switch) and after (post-switch) the change in dimensions is statistically significant. ```{r} remove(bySub,bySubWide,byGroup,ts,max_cluster) ``` ## Logistic Mixed Effects We now have all of the tools to repeat this process using a more complicated model: logistic mixed effects modeling. We're going to be fitting 46 of these models one for each time frame. We will keep the data at the individual trial level - there will be no aggregating so we'll be dealing with accuracies of 1's or 0's (e.g., was the participant looking at the target or the distractor image at time 300ms on trial 2). Hence the need to use logistic regression. We're going to use the `glmer()` function from the `lme4` package. We just need to make a few modifications to our functions to extract information from these models rather than t-tests (extracting z, rather than t values). ```{r echo=T} extract_glmer_z <- function(this_lm){ this_lm = as.data.frame(coef(summary(this_lm))) lm_table <- this_lm %>% mutate(term = rownames(this_lm)) %>% select(term, `z value`) %>% rename(z = `z value`) %>% filter(term != "(Intercept)") return(lm_table) } get_max_cluster_glmer <- function(m, threshold = 2){ colnames(m) <- c("time","term", "stat") m = as.data.frame(m) # Calculate all cluster sizes d_clust <- m %>% mutate( sig=ifelse(stat >= threshold, "pos", ifelse(stat <= -threshold, "neg", "ns")), cluster=data.table::rleid(sig) ) %>% filter(sig != "ns") %>% group_by(cluster) %>% summarise( start=min(time), stop=max(time), auc=sum(stat)) max_cluster=data.frame(cluster=0,start=0,stop=0,auc=0) if(nrow(d_clust) > 0) {max_cluster = d_clust[which.max(abs(d_clust$auc)),]} return(max_cluster) } ``` Here's a peak at what the data looks like: ```{r echo=T} bySub=d %>% filter(TimeC>=300 & TimeC <=1800 & percentMissingFrames < 50) %>% select(Sub.Num,Order,Tr.Num,Condition,TimeC,Accuracy) bySub$Condition <- factor(bySub$Condition, c('pre-switch','post-switch')) contrasts(bySub$Condition) = c(-.5,.5) colnames(attr(bySub$Condition,"contrasts")) = "" ``` ```{r} kable(head(bySub,20), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` ```{r} kable(head(bySub[bySub$TimeC==300,],20), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` And here's the syntax for repeating the glmer() model 46 times (1 for each time frame during our critical window): ```{r echo=T} zs <- bySub %>% group_by(TimeC) %>% group_modify(~extract_glmer_z(glmer(Accuracy ~ Condition + (Condition|Sub.Num), data=.x,family=binomial))) %>% mutate(Sig = ifelse(abs(z)>2,"yes","no")) ``` Here's what the output looks like: ```{r} kable(head(zs,20), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` Notice the massive jump in the z value for the 833 ms time frame? I've surpressed warnings when knitting the markdown file, but that model was generating the following error: `Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl=control$checkConv, : Model failed to converge with max|grad| = 0.0280114 (tol = 0.002, component 1)` Errors regarding convergence are serious. The approximation procedure may not have identified the correct parameters to minimize error and therefore the test of significance may not be appropriate. Let's check out that model for the frame 800 right before the problematic test: ```{r echo=T} temp=bySub[bySub$TimeC==800,] ``` ```{r} kable(head(temp,20), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` ```{r echo=T} m = glmer(Accuracy ~ Condition + (Condition|Sub.Num), data=temp,family=binomial) summary(m) ``` And now the test for 833 ms (the problematic one): ```{r echo=T} temp = bySub[bySub$TimeC==833,] m = glmer(Accuracy ~ Condition + (Condition|Sub.Num), data=temp,family=binomial) summary(m) ``` Let's refit this model without the random slope for condition: ```{r echo=T} m = glmer(Accuracy ~ Condition + (1|Sub.Num), data=temp,family=binomial) summary(m) ``` We are no longer getting a massive z value for the effect condition and its value is more in line with the tests for the other time frames. This hearkens back to our earlier discussion about how defaulting to always use the maximal random effects structure (including an intercept and then slope for every within-subject effect) may not always be appropriate. I am not quite sure what's happening here (I need to do some more digging), but I think it's reflective of our underlying problem that there's not enough data (as we'll see in a moment when we start fitting the models to randomized data) and our model fits are precarious (i.e., many will not converge). It's always important to check the errors and especially model results when you're getting these types of errors. Let's complete the series of 46 glmer models without the random slope: ```{r echo=T} zs <- bySub %>% group_by(TimeC) %>% group_modify(~extract_glmer_z(glmer(Accuracy ~ Condition + (1|Sub.Num), data=.x,family=binomial))) %>% mutate(Sig = ifelse(abs(z)>2,"yes","no")) ``` We're finding reasonable and consistent z values for all time frames (although we did get one (surpressed) convergence warning for our 3rd to last model fit) ```{r} kable(zs, align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` ```{r echo=T} ggplot(data=zs[zs$term=='Condition',],aes(x=TimeC,y=z,color=Sig)) + geom_point()+ theme_bw() + scale_color_manual(values=c("black","red")) + geom_hline(yintercept = 2) + geom_hline(yintercept = -2) + scale_x_continuous(breaks=seq(from=300,to=1800,by=300)) + labs(x='Time since target onset (in ms)',y='z values') ``` Let's extract the area under the curve for the largest cluster of significant effects, which is the same as when we used the t tests (with the exception that it extends one frame longer to 867ms) ```{r echo=T} max_cluster_condition=get_max_cluster_glmer(zs %>% select(-Sig) %>% filter(term=='Condition')) max_cluster_condition$term = 'Condition' # max_cluster=bind_rows(max_cluster_condition,max_cluster_group,max_cluster_interaction) %>% # select(auc,term) max_cluster=(max_cluster_condition) %>% select(auc,term) ``` ```{r} kable(max_cluster_condition, align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` ```{r echo=T} byGroup=d %>% filter(TimeC>=300 & TimeC <=1800 & percentMissingFrames < 50) %>% # select time window & max missing frames group_by(Sub.Num, Condition, TimeC) %>% # aggregate over trials in each condition for each subject summarise(Accuracy=mean(Accuracy,na.rm=T)) %>% group_by(Condition, TimeC) %>% # aggregate over participants in each group for each condition summarise( N=sum(!is.na(Accuracy)), SD=sd(Accuracy,na.rm=TRUE), SE=SD/sqrt(N), Accuracy=mean(Accuracy,na.rm=TRUE), lower=Accuracy-SE, upper=Accuracy+SE) byGroup$Condition = factor(byGroup$Condition, c("pre-switch","post-switch")) ggplot(byGroup, aes(x=TimeC, y=Accuracy, fill=Condition, color=Condition)) + geom_hline(aes(yintercept=0.5),linetype='solid',color='gray') + geom_line()+ geom_segment(x=max_cluster_condition$start,xend=max_cluster_condition$stop,y=.41,yend=.41,linetype='solid',color='black') + geom_smooth(aes(ymin=lower, ymax=upper), stat="identity") + geom_vline(aes(xintercept=0), linetype="dashed", color="gray") + theme_bw(base_size=10) + coord_cartesian(xlim=c(300,1800), ylim=c(.40,1.01),expand=F) + scale_x_continuous(breaks=seq(from=-900,to=2400,by=300)) + scale_y_continuous(breaks=seq(from=0,to=1,by=.1)) + scale_fill_manual(values=c("dodgerblue","coral2")) + scale_color_manual(values=c("dodgerblue","coral2")) + labs(x='Time since target onset (in ms)',y='Proportion Looking to Target',title='Condition Effect') + theme(legend.justification=c(1,0), plot.title=element_text(hjust=.5),legend.position=c(1,0),legend.background=element_rect(fill= NA, color=NA),legend.title=element_blank(),legend.text = element_text(size = 11)) ``` ### Randomization ```{r echo=T} bySub=d %>% filter(TimeC>=300 & TimeC <=1800 & percentMissingFrames < 50) %>% shuffConditions(3) %>% select(Sub.Num,Order,Tr.Num,CondR,TimeC,Accuracy) bySub$CondR <- factor(bySub$CondR, c('pre-switch','post-switch')) contrasts(bySub$CondR) = c(-.5,.5) colnames(attr(bySub$CondR,"contrasts")) = "" zs_rand1 <- bySub %>% group_by(TimeC) %>% group_modify(~extract_glmer_z(glmer(Accuracy ~ CondR + (1|Sub.Num), data=.x,family=binomial))) %>% mutate(Sig = ifelse(abs(z)>2,"yes","no")) max_cluster_rand=get_max_cluster_glmer(zs_rand1 %>% select(-Sig) %>% filter(term=='CondR')) max_cluster_rand$term = 'CondR' ``` ```{r} kable(zs_rand1, align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` ```{r echo=T} ggplot(data=zs_rand1[zs_rand1$term=='CondR',],aes(x=TimeC,y=z,color=Sig)) + geom_point()+ theme_bw() + scale_color_manual(values=c("black","red")) + geom_hline(yintercept = 2) + geom_hline(yintercept = -2) + scale_x_continuous(breaks=seq(from=300,to=1800,by=300)) + labs(x='Time since target onset (in ms)',y='z values') ``` One of our glmer() model fits returned a massive significant effect: ```{r} max_cluster_rand ``` This is a large effect size for a time frame (1300) in our data where there is not a large difference between our randomized conditions: ```{r echo=T} byGroup=bySub %>% group_by(CondR, TimeC) %>% # aggregate over participants in each group for each condition summarise( N=sum(!is.na(Accuracy)), SD=sd(Accuracy,na.rm=TRUE), SE=SD/sqrt(N), Accuracy=mean(Accuracy,na.rm=TRUE), lower=Accuracy-SE, upper=Accuracy+SE) byGroup$CondR = factor(byGroup$CondR, c("pre-switch","post-switch")) ggplot(byGroup, aes(x=TimeC, y=Accuracy, fill=CondR, color=CondR)) + geom_hline(aes(yintercept=0.5),linetype='solid',color='gray') + geom_line()+ geom_segment(x=max_cluster_condition$start,xend=max_cluster_condition$stop,y=.41,yend=.41,linetype='solid',color='black') + geom_smooth(aes(ymin=lower, ymax=upper), stat="identity") + geom_vline(aes(xintercept=0), linetype="dashed", color="gray") + theme_bw(base_size=10) + coord_cartesian(xlim=c(300,1800), ylim=c(.40,1.01),expand=F) + scale_x_continuous(breaks=seq(from=-900,to=2400,by=300)) + scale_y_continuous(breaks=seq(from=0,to=1,by=.1)) + scale_fill_manual(values=c("dodgerblue","coral2")) + scale_color_manual(values=c("dodgerblue","coral2")) + labs(x='Time since target onset (in ms)',y='Proportion Looking to Target',title='Condition Randomized') + theme(legend.justification=c(1,0), plot.title=element_text(hjust=.5),legend.position=c(1,0),legend.background=element_rect(fill= NA, color=NA),legend.title=element_blank(),legend.text = element_text(size = 11)) ``` As before, this model is failing to converge: ```{r echo=T} temp=d %>% filter(TimeC==1300 & percentMissingFrames < 50) %>% shuffConditions(3) %>% select(Sub.Num,Order,Tr.Num,CondR,TimeC,Accuracy) temp$CondR <- factor(temp$CondR, c('pre-switch','post-switch')) contrasts(temp$CondR) = c(-.5,.5) colnames(attr(temp$CondR,"contrasts")) = "" m = glmer(Accuracy ~ CondR + (1|Sub.Num), data=temp,family=binomial) summary(m) ``` This is concerning, because it suggests that our data (i.e., 8 trials per condition per participant) may not be sufficient for logistic mixed effects models to converge. We can confirm this by seeing how often we find large effects in our 1,000 randomizations. ### Randomize x 1000 We're using the same `foreach` and `%dopar%` syntax to re-reun the updated code to fit the glmer() models 1,000 times. These models fit fairly rapidly, but still take longer than t tests. This adds up when running 46 tests 1,000 times. It took my computer just under 8 minutes to complete this process: ```{r echo=T} # nrep=1000 # pb <- txtProgressBar(max=nrep, style=3) # progress <- function(n) setTxtProgressBar(pb, n) # opts <- list(progress=progress) # # startTime=format(Sys.time(), "%X") # # zs_randCond=foreach (i = 1:nrep, .combine = rbind, .inorder=F, .options.snow = opts, .packages = c("tidyverse","purrr")) %dopar% { # # bySub=d %>% # filter(TimeC>=300 & TimeC <=1800 & percentMissingFrames < 50) %>% # shuffConditions(i) %>% # select(Sub.Num,Order,Tr.Num,CondR,TimeC,Accuracy) # # bySub$CondR <- factor(bySub$CondR, c('pre-switch','post-switch')) # contrasts(bySub$CondR) = c(-.5,.5) # colnames(attr(bySub$CondR,"contrasts")) = "" # # zs <- bySub %>% # group_by(TimeC) %>% # group_modify(~extract_glmer_z(glmer(Accuracy ~ CondR + (1|Sub.Num), data=.x,family=binomial))) %>% # mutate(Sig = ifelse(abs(z)>2,"yes","no")) # # max_cluster_condition=get_max_cluster_glmer(zs %>% select(-Sig) %>% filter(term=='CondR')) # max_cluster_condition$term = 'CondR' # max_cluster=(max_cluster_condition) %>% select(auc,term) # # return(max_cluster) # # } # # stopTime=format(Sys.time(), "%X") ``` ```{r} # save(zs_randCond,file='zs_randCond.Rdata') load('zs_randCond.Rdata') ``` ```{r} kable(head(zs_randCond %>% arrange(auc),20), align='c',digits=2) %>% kable_styling(bootstrap_options = c("hover", "condensed", "responsive"),full_width=F,position='left') ``` Yeah, there are a lot of very large auc values. So much so, that the significant effect we observed when using t tests is not statistically significant when using the logistic mixed effects models: ```{r echo=T} p_obs <- mean(abs(zs_randCond$auc)> abs(max_cluster_condition$auc)) t_crit_pos <- quantile(zs_randCond$auc, probs = 0.975) t_crit_neg <- quantile(zs_randCond$auc, probs = 0.025) ggplot(mapping = aes(x = zs_randCond$auc)) + geom_density(fill = "green", alpha = .5) + geom_vline(xintercept = t_crit_pos) + geom_vline(xintercept = t_crit_neg) + geom_vline(xintercept = max_cluster_condition$auc, colour = "blue") + labs( title = "Condition Effect, 300-1800 ms", x = "Largest Cluster (1000 Permutations)" ) ``` With the raw data, children's accuracy in fixating the target object was higher for pre-switch compared to post-switch trials during a window from `r max_cluster_condition$start`ms to `r max_cluster_condition$stop`ms after the onset of the critical word. The size of this effect (i.e., area under the curve) is `r round(max_cluster_condition$auc,digits=2)`. When reshuffling the data (randomly re-assigning trials for a child to be in the pre- vs. post-switch condition), we observe an effect of this size `r paste(p_obs*100,'%',sep='')` of the time. What these results suggest is that 8 observations per condition (16 total) for each participant is not sufficient for logistic mixed effects modeling. For many of the time frames, the model fit was able to converge, but the model failed to converge a non-trivial number of times. We can see this mostly clearly in the distribution of auc values. I have successfully used logistic mixed effects models for cluster-based permutation analyses for another data set. That data set, however, contained 24 trials per condition (48 total). So, if you need to use logistic mixed effects modeling for cluster-based permutation analyses (e.g., you have multiple fixed effects or want to include a cognitive measure as a covariate) then you should plan to have more than 8 trials for each level of your fixed effect.